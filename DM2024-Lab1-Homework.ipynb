{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:龔祺竣\n",
    "\n",
    "Student ID:D1124181010\n",
    "\n",
    "GitHub ID:anku0910"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: do the **take home** exercises in the [DM2024-Lab1-Master](https://github.com/didiersalazar/DM2024-Lab1-Master.git). You may need to copy some cells from the Lab notebook to this notebook. __This part is worth 20% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: follow the same process from the [DM2024-Lab1-Master](https://github.com/didiersalazar/DM2024-Lab1-Master.git) on **the new dataset**. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 30% of your grade.__\n",
    "    - Download the [the new dataset](https://huggingface.co/datasets/Senem/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data). The dataset contains a `sentiment` and `comment` columns, with the sentiment labels being: 'nostalgia' and 'not nostalgia'. Read the specificiations of the dataset for background details. \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 30% of your grade.__\n",
    "    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency). Refer to this Scikit-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n",
    "    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Note that for the TF-IDF features you might need to use other type of NB classifier different than the one in the Master Notebook. Comment on the differences.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n",
    "\n",
    "\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be handled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "5. Fifth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/didiersalazar/DM2024-Lab1-Master/blob/main/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb). Make sure to commit and save your changes to your repository __BEFORE the deadline (October 27th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T23:50:57.474287Z",
     "start_time": "2024-10-11T23:50:57.461278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['He was a singer with a golden voice that I love to hear all the time. I was his great fan at the age of 16years in those days and still now. Although we have many singers now, but, I can vouch for Jim Reeves all the time. You feel relaxed, emotional and loving. Thank you Lord for his life.',\n",
       "       'The mist beautiful voice ever I listened to him when I was a kid and still love his singing Never forgotten The voice of an angel'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "data = pd.read_csv('senem/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data.csv')  # Load\n",
    "categories = data['sentiment'].unique()  # Extract sentiment categories\n",
    "\n",
    "data['sentiment'] = pd.Categorical(data['sentiment'], categories=categories)  # Convert sentiment column to categorical\n",
    "data['sentiment'] = data['sentiment'].cat.codes  # Convert to numerical code\n",
    "\n",
    "X = data['comment'].values \n",
    "y = data['sentiment'].values  \n",
    "\n",
    "sentiment1 = Bunch(data=X, target=y, feature_names=['text'], target_names=categories)  # Create a structured dataset for view\n",
    "sentiment1.data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not nostalgia\n",
      "nostalgia\n",
      "nostalgia\n",
      "not nostalgia\n",
      "nostalgia\n",
      "nostalgia\n",
      "nostalgia\n",
      "nostalgia\n",
      "not nostalgia\n",
      "not nostalgia\n"
     ]
    }
   ],
   "source": [
    "for t in sentiment1.target[:10]:\n",
    "    print(sentiment1.target_names[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiment1.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 1 (Watch Video):**  \n",
    "In this exercise, please print out the *text* data for the first three samples in the dataset. (See the above code for help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was a singer with a golden voice that I love to hear all the time. I was his great fan at the age of 16years in those days and still now. Although we have many singers now, but, I can vouch for Jim Reeves all the time. You feel relaxed, emotional and loving. Thank you Lord for his life.\n",
      "The mist beautiful voice ever I listened to him when I was a kid and still love his singing Never forgotten The voice of an angel\n",
      "I have most of Mr. Reeves songs.  Always love his smooth voice, so comforting and sounds like the people I grew up around.  They were so gentle and sweet, but I had to leave my little town to find a job at age 17.  I graduated early.  I never forgot how great it was to grow up with simple people who had no pretensions -- just loving and gentle, like I think Jesus is.\n"
     ]
    }
   ],
   "source": [
    "# Answer here\n",
    "for text in sentiment1.data[:3]:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Converting Dictionary into Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T23:50:57.599198Z",
     "start_time": "2024-10-11T23:50:57.592204Z"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (data_mining_helpers.py, line 43)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\anku0\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3457\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\anku0\\AppData\\Local\\Temp\\ipykernel_23352\\3211219035.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    import helpers.data_mining_helpers as dmh\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\anku0\\Documents\\GitHub\\DM2024-Lab1-Homework\\helpers\\data_mining_helpers.py\"\u001b[1;36m, line \u001b[1;32m43\u001b[0m\n\u001b[1;33m    def format_rows(docs):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import helpers.data_mining_helpers as dmh\n",
    "\n",
    "X = pd.DataFrame.from_records(dmh.format_rows(sentiment1), columns= ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T23:50:57.646326Z",
     "start_time": "2024-10-11T23:50:57.632317Z"
    }
   },
   "outputs": [],
   "source": [
    "X['category'] = sentiment1.target\n",
    "X['category_name'] = X.category.apply(lambda t: dmh.format_labels(t, sentiment1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T23:50:57.708289Z",
     "start_time": "2024-10-11T23:50:57.681287Z"
    }
   },
   "outputs": [],
   "source": [
    "X[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Familiarizing yourself with the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple query\n",
    "X[:10][[\"text\",\"category_name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using loc (by label)\n",
    "X.loc[:10, 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 2 (take home):** \n",
    "Experiment with other querying techniques using pandas dataframes. Refer to their [documentation](https://pandas.pydata.org/pandas-docs/stable/indexing.html) for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer here\n",
    "print(X[X['category_name']!='nostalgia'].iloc[::10][0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 3 (Watch Video):**  \n",
    "Try to fetch records belonging to the ```sci.med``` category, and query every 10th record. Only show the first 5 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "print(X[X['category_name']=='nostalgia'].iloc[::10][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Mining using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T23:50:57.817240Z",
     "start_time": "2024-10-11T23:50:57.803247Z"
    }
   },
   "outputs": [],
   "source": [
    "X.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 4 (Watch Video):** \n",
    "Let's try something different. Instead of calculating missing values by column let's try to calculate the missing values in every record instead of every column.  \n",
    "$Hint$ : `axis` parameter. Check the documentation for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "X.isnull().apply(lambda X: dmh.check_missing_values(X),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 5 (take home)** \n",
    "$Hint$ :  why `.isnull()` didn't work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Dealing with Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T23:50:57.957159Z",
     "start_time": "2024-10-11T23:50:57.945167Z"
    }
   },
   "outputs": [],
   "source": [
    "sum(X.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(X.duplicated('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T23:50:58.128062Z",
     "start_time": "2024-10-11T23:50:58.108073Z"
    }
   },
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T23:50:58.285041Z",
     "start_time": "2024-10-11T23:50:58.267722Z"
    }
   },
   "outputs": [],
   "source": [
    "X.drop_duplicates(keep=\"first\", inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T23:50:58.378978Z",
     "start_time": "2024-10-11T23:50:58.365986Z"
    }
   },
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(X.duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.category_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled = X.sample(n=450)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to plot the category distribution for both original and resampled data \n",
    "def plot_category_distribution(original_data, resampled_data):\n",
    "    \"\"\" \n",
    "    Function to plot the category distribution for both original and resampled data \n",
    "    \"\"\"\n",
    "    # Convert data to DataFrame\n",
    "    original_df = pd.DataFrame(original_data, columns=[\"category\"])\n",
    "    resampled_df = pd.DataFrame(resampled_data, columns=[\"category\"])\n",
    "\n",
    "    # Calculate value counts for original and resampled data\n",
    "    original_count = original_df['category'].value_counts()\n",
    "    resampled_count = resampled_df['category'].value_counts()\n",
    "\n",
    "    # Define categories and index for plotting\n",
    "    categories = original_count.index.tolist()  # Extract categories from the data\n",
    "    index = np.arange(len(categories))\n",
    "\n",
    "    # Create a bar plot for original and resampled data\n",
    "    plt.bar(index, original_count.values, width=0.2, label='Original', align='center')\n",
    "    plt.bar(index + 0.2, resampled_count.values, width=0.2, label='Resampled', align='center')\n",
    "\n",
    "    # Set x-axis ticks and labels\n",
    "    plt.xticks(index + 0.15, categories)\n",
    "    plt.xlabel('Categories')\n",
    "    plt.title('Category Distribution: Original vs Resampled')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_category_distribution(X, X_resampled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "ngram_range=(1,1) \n",
    "X['unigrams'] = X['text'].apply(CountVectorizer(ngram_range=ngram_range).build_analyzer()) \n",
    "# Use built-in tokenizer \n",
    "list(X[0:1]['unigrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Feature subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize and fit the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "tdm = vectorizer.fit_transform(X['text'])  # Fit with the 'text' column of X\n",
    "\n",
    "# Extract feature names and index labels\n",
    "plot_x = [\"term_\" + str(i) for i in vectorizer.get_feature_names_out()[0:20]]\n",
    "plot_y = [\"doc_\" + str(i) for i in range(20)]  # Generate labels for first 20 documents\n",
    "plot_z = tdm[0:20, 0:20].toarray()  # Select first 20 documents and first 20 terms\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "tdm_df = pd.DataFrame(plot_z, columns=plot_x, index=plot_y)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(tdm_df, cmap='RdPu', vmin=0, vmax=tdm_df.max().max(), annot=False, cbar_kws={'label': 'Term Frequency'})\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Atrribute Transformation / Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Function to plot term frequencies with optional log scale and sorting\n",
    "def plot_term_frequencies(tdm_df, start_range=0, end_range=100, logScale=False, ascending=None):\n",
    "    term_frequencies = tdm_df.sum(axis=0).to_numpy()  # Sum term frequencies across documents\n",
    "    terms = tdm_df.columns.to_list()  # Extract column names (terms)\n",
    "    \n",
    "    x_values = np.asarray(terms)\n",
    "    y_values = term_frequencies\n",
    "    \n",
    "    # Apply log scale if needed\n",
    "    if logScale:\n",
    "        y_values = np.log(y_values + 1)  # Log scale (add 1 to avoid log(0))\n",
    "\n",
    "    # Sort terms based on frequency if ascending/descending is specified\n",
    "    if ascending is not None:\n",
    "        sorted_indices = np.argsort(y_values)\n",
    "        if not ascending:\n",
    "            sorted_indices = sorted_indices[::-1]  # Reverse for descending order\n",
    "        x_values = x_values[sorted_indices]\n",
    "        y_values = y_values[sorted_indices]\n",
    "\n",
    "    # Plot term frequencies within the specified range\n",
    "    fig = go.Figure(go.Bar(x=x_values[start_range:end_range], y=y_values[start_range:end_range]))\n",
    "    \n",
    "    # Update layout for better readability and interactivity\n",
    "    fig.update_layout(\n",
    "        width=700,\n",
    "        height=400,\n",
    "        xaxis=dict(tickangle=-90),\n",
    "        sliders=[{\n",
    "            'active': 0,\n",
    "            'currentvalue': {\"prefix\": \"Range: \"},\n",
    "            'steps': [\n",
    "                {'label': f'{i}-{i+end_range-1}',\n",
    "                 'method': 'update',\n",
    "                 'args': [{'x': [x_values[i:i+end_range]], 'y': [y_values[i:i+end_range]]}]}\n",
    "                for i in range(0, len(x_values), end_range)  # Step through terms in batches of `end_range`\n",
    "            ]\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "plot_term_frequencies(tdm_df, start_range=0, end_range=100, logScale=False, ascending=None)  # Basic plot\n",
    "plot_term_frequencies(tdm_df, start_range=0, end_range=100, logScale=True, ascending=None)  # Log scale plot\n",
    "plot_term_frequencies(tdm_df, start_range=0, end_range=100, logScale=False, ascending=False)  # Sorted by frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding frequent patterns for each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting texts for each category to a term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create separate DataFrames for each category\n",
    "category_dfs = {}  # Dictionary to store DataFrames for each category\n",
    "for category in categories:\n",
    "    # Filter the original DataFrame by category\n",
    "    category_dfs[category] = X[X['category_name'] == category].copy()\n",
    "\n",
    "# Create term-document frequency DataFrames for each category\n",
    "term_document_dfs = {}  # Dictionary to store term-document DataFrames for each category\n",
    "for category in categories:\n",
    "    term_document_dfs[category] = dmh.create_term_document_df(category_dfs[category]['text'])\n",
    "category_name = categories[0]\n",
    "category_word_counts = term_document_dfs[category_name].sum(axis=0).to_numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(category_word_counts, bins=700, color='blue', edgecolor='black')\n",
    "plt.title(f'Term Frequency Distribution for Category {category_name}')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Number of Terms')\n",
    "plt.xlim(1, 200)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bottom 1% and top 5% words from term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers.text_analysis as ta\n",
    "\n",
    "filt_term_document_dfs = {}\n",
    "stop_words_dict = {}\n",
    "\n",
    "# Step 1: Apply filtering and collect stop words for all categories in one go\n",
    "for category in categories:\n",
    "    filt_term_document_dfs[category], stop_words_dict[category] = dmh.filter_top_bottom_words_by_sum(\n",
    "        term_document_dfs[category], verbose=False)\n",
    "\n",
    "# Step 2: Aggregate stop words into a single list for later use\n",
    "stop_words_list = [item for sublist in stop_words_dict.values() for item in sublist]\n",
    "\n",
    "# Step 3: Plot term frequencies for each filtered category\n",
    "for category_name, filt_tdm in filt_term_document_dfs.items():\n",
    "    print(f\"Plot of Term Frequencies for Category {category_name}\")\n",
    "    ta.plot_term_frequencies(\n",
    "        filt_term_document_dfs[category_name], start_range=0, end_range=100, logScale=False, ascending=False)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Explanation:\n",
    "# The dictionary comprehension handles filtering in one line, creating filt_term_document_dfs and populating stop_words_dict.\n",
    "# sum is used to concatenate stop word lists, reducing nested list comprehensions.\n",
    "# plot_term_frequencies is called directly in the loop for each category without redundant references.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data.columns)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the filtered term-document matrix to transactional database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PAMI.extras.convert.DF2DB import DF2DB\n",
    "\n",
    "# Loop through the dictionary of term-document DataFrames\n",
    "for category in filt_term_document_dfs:\n",
    "    # Replace dots with underscores in the category name to avoid errors in the file creation\n",
    "    category_safe = category.replace(' ', '_')\n",
    "    \n",
    "    # Create the DenseFormatDF object and convert to a transactional database\n",
    "    obj = DF2DB(filt_term_document_dfs[category])\n",
    "    obj.convert2TransactionalDatabase(f'td_freq_db_{category_safe}.csv', '>=', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing and visualizing stats for transactional database (for determining support threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PAMI.extras.dbStats import TransactionalDatabase\n",
    "obj = TransactionalDatabase.TransactionalDatabase('td_freq_db_nostalgia.csv')\n",
    "obj.run()\n",
    "obj.printStats()\n",
    "obj.plotGraphs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_obj = TransactionalDatabase.TransactionalDatabase('td_freq_db_not_nostalgia.csv')\n",
    "db_obj.run()\n",
    "db_obj.printStats()\n",
    "db_obj.plotGraphs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply FPGrowth algorithms to finding frequent pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PAMI.frequentPattern.basic import FPGrowth as alg\n",
    "\n",
    "# nostalgia data\n",
    "fp_obj_nostalgia = alg.FPGrowth(iFile='td_freq_db_nostalgia.csv', minSup=9)\n",
    "fp_obj_nostalgia.mine()\n",
    "print('Total Patterns Count for Nostalgia:', len(fp_obj_nostalgia.getPatternsAsDataFrame()))\n",
    "print('Runtime for Nostalgia:', fp_obj_nostalgia.getRuntime())\n",
    "fp_obj_nostalgia.save('freq_patterns_nostalgia_minSup9.txt')\n",
    "\n",
    "ta.plot_frequent_patterns(patterns_df_nostalgia,ascending=9)\n",
    "\n",
    "# not nostalgia data\n",
    "fp_obj_not_nostalgia = alg.FPGrowth(iFile='td_freq_db_not_nostalgia.csv', minSup=9)\n",
    "fp_obj_not_nostalgia.mine()\n",
    "print('Total Patterns Count for Not Nostalgia:', len(fp_obj_not_nostalgia.getPatternsAsDataFrame()))\n",
    "print('Runtime for Not Nostalgia:', fp_obj_not_nostalgia.getRuntime())\n",
    "fp_obj_not_nostalgia.save('freq_patterns_not_nostalgia_minSup9.txt')\n",
    "\n",
    "ta.plot_frequent_patterns(patterns_df_not_nostalgia,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nrclex import NRCLex\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load patterns from files\n",
    "def load_patterns(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return [line.split(\":\")[0].strip() for line in f]\n",
    "\n",
    "nostalgia_data = load_patterns(\"freq_patterns_nostalgia_minSup9.txt\")\n",
    "not_nostalgia_data = load_patterns(\"freq_patterns_not_nostalgia_minSup9.txt\")\n",
    "\n",
    "# Analyze emotional words with NRCLex\n",
    "def analyze_emotions(words):\n",
    "    emotions = {emotion: [] for emotion in ['positive', 'negative', 'joy', 'sadness', 'anger', \n",
    "                                            'anticipation', 'trust', 'fear', 'surprise', 'disgust']}\n",
    "    for word in words:\n",
    "        analysis = NRCLex(word)\n",
    "        for emotion in emotions:\n",
    "            if emotion in analysis.raw_emotion_scores:\n",
    "                emotions[emotion].append(word)\n",
    "    return {k: len(v) for k, v in emotions.items()}\n",
    "\n",
    "# Count temporal words\n",
    "temporal_words = [\"yesterday\", \"today\", \"tomorrow\", \"night\", \"year\", \"past\", \"future\", \"now\", \"ago\"]\n",
    "\n",
    "def count_temporal(words, temporal_list):\n",
    "    return {word: words.count(word) for word in temporal_list if word in words}\n",
    "\n",
    "nostalgia_time = count_temporal(nostalgia_data, temporal_words)\n",
    "not_nostalgia_time = count_temporal(not_nostalgia_data, temporal_words)\n",
    "\n",
    "# Extract adjectives /using POS tagging\n",
    "def extract_adj(words):\n",
    "    all_adjectives = []\n",
    "    for phrase in words:\n",
    "        tokens = word_tokenize(phrase)\n",
    "        tags = pos_tag(tokens)\n",
    "        all_adjectives.extend([word for word, tag in tags if tag == 'JJ'])  # 'JJ' is the POS tag for adjectives\n",
    "    return Counter(all_adjectives)\n",
    "\n",
    "nostalgia_adjectives = extract_adj(nostalgia_data)\n",
    "not_nostalgia_adjectives = extract_adj(not_nostalgia_data)\n",
    "\n",
    "# Visual\n",
    "def show_wordcloud(word_freq, title):\n",
    "    wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Gen clouds\n",
    "show_wordcloud(nostalgia_time, \"Nostalgia - Temporal References\")\n",
    "show_wordcloud(not_nostalgia_time, \"Not Nostalgia - Temporal References\")\n",
    "show_wordcloud(nostalgia_adjectives, \"Nostalgia - Descriptive Adjectives\")\n",
    "show_wordcloud(not_nostalgia_adjectives, \"Not Nostalgia - Descriptive Adjectives\")\n",
    "\n",
    "# Findings\n",
    "print(\"N Emotional Words:\", analyze_emotions(nostalgia_data))\n",
    "print(\"NN Emotional Words:\", analyze_emotions(not_nostalgia_data))\n",
    "print(\"\\nN Temporal References:\", nostalgia_time)\n",
    "print(\"NN Temporal References:\", not_nostalgia_time)\n",
    "print(\"\\nNN Descriptive Adjectives:\", nostalgia_adjectives.most_common(10))\n",
    "print(\"NN Descriptive Adjectives:\", not_nostalgia_adjectives.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each category, filter the patterns to keep only the unique ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfs = [patterns_df_nostalgia, patterns_df_not_nostalgia]\n",
    "pattern_freq = {}\n",
    "\n",
    "# Count pattern occurrences across both files\n",
    "for df in dfs:\n",
    "    for pattern in df['Patterns']:\n",
    "        pattern_freq[pattern] = pattern_freq.get(pattern, 0) + 1\n",
    "\n",
    "# Filter out something that appearing more than once\n",
    "unique_only = {pattern for pattern, count in pattern_freq.items() if count == 1}\n",
    "\n",
    "# Compute total patterns and discarded ones\n",
    "total_count = sum(len(df) for df in dfs)\n",
    "discarded_count = total_count - len(unique_only)\n",
    "\n",
    "# Keep only unique patterns in final DataFrame\n",
    "filtered_data = [df[df['Patterns'].isin(unique_only)] for df in dfs]\n",
    "final_df = pd.concat(filtered_data)\n",
    "\n",
    "ta.plot_frequent_patterns(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_patterns(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return [line.split(\":\")[0].strip() for line in f.readlines()]\n",
    "\n",
    "nostalgia_data = load_patterns(\"freq_patterns_nostalgia_minSup9.txt\")\n",
    "not_nostalgia_data = load_patterns(\"freq_patterns_not_nostalgia_minSup9.txt\")\n",
    "\n",
    "# Load patterns\n",
    "common = set(nostalgia_data) & set(not_nostalgia_data)\n",
    "unique_nostalgia = set(nostalgia_data) - common\n",
    "unique_not_nostalgia = set(not_nostalgia_data) - common\n",
    "\n",
    "emotional_words = [ \"sad\",\"memory\", \"happy\", \"cry\", \"tears\", \"gone\", \"feel\"]\n",
    "temporal_words = [\"youth\", \"yesterday\", \"today\", \"growing\", \"1963\", \"forever\"]\n",
    "\n",
    "def analyze(words, target):\n",
    "    return {word: words.count(word) for word in target if word in words}\n",
    "\n",
    "nostalgia_emotions = analyze(nostalgia_data, emotional_words)\n",
    "not_nostalgia_emotions = analyze(not_nostalgia_data, emotional_words)\n",
    "\n",
    "def WordCloud(freq, title):\n",
    "    wc = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(freq)\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "# result\n",
    "WordCloud(nostalgia_emotions, \"Nostalgia - Emotional Words\")\n",
    "WordCloud(not_nostalgia_emotions, \"Not Nostalgia - Emotional Words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the texts into the frequency-based term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert 'text' column\n",
    "tokenizer = CountVectorizer().build_analyzer()\n",
    "\n",
    "# Generate term-document matrix for the 'text' column with consistent indexing\n",
    "tdm_df = dmh.create_term_document_df(X['text'], vectorizer=CountVectorizer(), index=X.index)\n",
    "tdm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the texts into the TF-IDF-based term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Generate tf-idf term-document matrix with consistent indexing for merging\n",
    "tfidf_tdm_df = dmh.create_term_document_df(X['text'], vectorizer=TfidfVectorizer(), index=X.index)\n",
    "tfidf_tdm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the unique patterns into the 0/1 patterm-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text into sets of words\n",
    "X['words'] = X['text'].apply(tokenizer).apply(set)\n",
    "\n",
    "# Initialize pattern matrix with zeroes\n",
    "pattern_matrix = pd.DataFrame(0, index=X.index, columns=final_pattern_df['Patterns'])\n",
    "\n",
    "# Fill pattern matrix: set to 1 if all pattern words exist in text\n",
    "for pattern in final_pattern_df['Patterns']:\n",
    "    pattern_words = set(pattern.split())\n",
    "    pattern_matrix[pattern] = X['words'].apply(lambda w: int(pattern_words.issubset(w)))\n",
    "\n",
    "pattern_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate the frequence-based term-document matrix and the pattern matrix to augment the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the term-document matrix with the pattern matrix to expand features\n",
    "augmented_tdm_df = pd.concat([tdm_df, pattern_matrix], axis=1)\n",
    "augmented_tdm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate the tf-idf-based term-document matrix and the pattern matrix to augment the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the original TDM and the pattern matrix to augment the features\n",
    "augmented_tfidf_tdm_df = pd.concat([tfidf_tdm_df, pattern_matrix], axis=1)\n",
    "augmented_tfidf_tdm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Function for 2D scatter plots\n",
    "def plot_scatter_2d(ax, data_reduced, title, colors, categories):\n",
    "    for color, category in zip(colors, categories):\n",
    "        x_vals = data_reduced[X['category_name'] == category, 0]\n",
    "        y_vals = data_reduced[X['category_name'] == category, 1]\n",
    "        ax.scatter(x_vals, y_vals, c=color, marker='o', label=category)   \n",
    "    ax.grid()\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "# Function for 3D scatter plots\n",
    "def plot_scatter_3d(ax, data_reduced, title, colors, categories):\n",
    "    for color, category in zip(colors, categories):\n",
    "        x_vals = data_reduced[X['category_name'] == category, 0]\n",
    "        y_vals = data_reduced[X['category_name'] == category, 1]\n",
    "        z_vals = data_reduced[X['category_name'] == category, 2]\n",
    "        ax.scatter(x_vals, y_vals, z_vals, c=color, marker='o', label=category)\n",
    "    \n",
    "    ax.grid(color='gray', linestyle=':', linewidth=2, alpha=0.2)\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "# Apply PCA, t-SNE, and UMAP to term-document data\n",
    "X_pca_tdm = PCA(n_components=2).fit_transform(tdm_df.values)\n",
    "X_tsne_tdm = TSNE(n_components=2).fit_transform(tdm_df.values)\n",
    "X_umap_tdm = umap.UMAP(n_components=2).fit_transform(tdm_df.values)\n",
    "\n",
    "# Plotting PCA, t-SNE, and UMAP in subplots\n",
    "colors = ['blue','orange']  # Colors for categories\n",
    "categories = X['category_name'].unique()  # Unique categories for labeling\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))  # Subplots for the 3 reduction methods\n",
    "\n",
    "# Plot results for each method\n",
    "plot_scatter_2d(axes[0], X_pca_tdm, 'PCA', colors=colors, categories=categories)\n",
    "plot_scatter_2d(axes[1], X_tsne_tdm, 't-SNE', colors=colors, categories=categories)\n",
    "plot_scatter_2d(axes[2], X_umap_tdm, 'UMAP', colors=colors, categories=categories)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dimensionality reduction to the augmented frequency-based term-document matrix and keep 2 components only\n",
    "# Function for 2D scatter plots\n",
    "def plot_scatter_2d(ax, data_reduced, title, colors, categories):\n",
    "    for color, category in zip(colors, categories):\n",
    "        x_vals = data_reduced[X['category_name'] == category, 0]\n",
    "        y_vals = data_reduced[X['category_name'] == category, 1]\n",
    "        ax.scatter(x_vals, y_vals, c=color, marker='o', label=category)\n",
    "    \n",
    "    ax.grid(color='gray', linestyle=':', linewidth=2, alpha=0.2)\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "# Apply PCA, t-SNE, and UMAP to augmented term-document matrix\n",
    "X_pca_tdm_aug = PCA(n_components=2).fit_transform(augmented_tdm_df.values)\n",
    "X_tsne_tdm_aug = TSNE(n_components=2).fit_transform(augmented_tdm_df.values)\n",
    "X_umap_tdm_aug = umap.UMAP(n_components=2).fit_transform(augmented_tdm_df.values)\n",
    "\n",
    "# Plotting PCA, t-SNE, and UMAP in subplots\n",
    "colors = ['blue','orange']  # Colors for categories\n",
    "categories = X['category_name'].unique()  # Unique categories for labeling\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))  # Subplots for the 3 reduction methods\n",
    "\n",
    "# Plot results for each method\n",
    "plot_scatter_2d(axes[0], X_pca_tdm_aug, 'PCA', colors=colors, categories=categories)\n",
    "plot_scatter_2d(axes[1], X_tsne_tdm_aug, 't-SNE', colors=colors, categories=categories)\n",
    "plot_scatter_2d(axes[2], X_umap_tdm_aug, 'UMAP', colors=colors, categories=categories)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dimensionality reduction to the frequency-based term-document matrix with 3 components\n",
    "X_pca = PCA(n_components=3).fit_transform(tdm_df.values)\n",
    "X_tsne = TSNE(n_components=3).fit_transform(tdm_df.values)\n",
    "X_umap = umap.UMAP(n_components=3).fit_transform(tdm_df.values)\n",
    "\n",
    "# Apply dimensionality reduction to the augmented term-document matrix with 3 components\n",
    "X_pca_aug = PCA(n_components=3).fit_transform(augmented_tdm_df.values)\n",
    "X_tsne_aug = TSNE(n_components=3).fit_transform(augmented_tdm_df.values)\n",
    "X_umap_aug = umap.UMAP(n_components=3).fit_transform(augmented_tdm_df.values)\n",
    "\n",
    "colors = ['coral', 'blue', 'black', 'orange']\n",
    "categories = X['category_name'].unique()\n",
    "\n",
    "# Function to create 3D scatter plot\n",
    "def plot_scatter_3d(ax, data, title, colors, categories):\n",
    "    for color, category in zip(colors, categories):\n",
    "        xs = data[X['category_name'] == category, 0]\n",
    "        ys = data[X['category_name'] == category, 1]\n",
    "        zs = data[X['category_name'] == category, 2]\n",
    "        ax.scatter(xs, ys, zs, c=color, marker='o', label=category)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "# Create and display 3D scatter plots for the original and augmented matrices\n",
    "for data, title_prefix in [(X_pca, \"Original PCA\"), (X_tsne, \"Original t-SNE\"), (X_umap, \"Original UMAP\"),\n",
    "                           (X_pca_aug, \"Augmented PCA\"), (X_tsne_aug, \"Augmented t-SNE\"), (X_umap_aug, \"Augmented UMAP\")]:\n",
    "    fig = plt.figure(figsize=(24, 8))\n",
    "    fig.suptitle(f'{title_prefix} Comparison')\n",
    "\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    plot_scatter_3d(ax, data, title_prefix, colors=colors, categories=categories)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Discretization and Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Initialize LabelBinarizer and fit it to the category labels\n",
    "label_binarizer = preprocessing.LabelBinarizer()\n",
    "label_binarizer.fit(X.category)\n",
    "\n",
    "# Transform the category column and assign it to a new column\n",
    "X['bin_category'] = label_binarizer.transform(X['category']).tolist()\n",
    "\n",
    "# Display the first 2 rows\n",
    "X[0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer.fit(X.category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Retrieve and transform 3 random sentences into vectors\n",
    "random_indices = [50, 100, 150]\n",
    "documents_to_transform = [X.iloc[i]['text'] for i in random_indices]\n",
    "document_vectors = [count_vectorizer.transform([doc]) for doc in documents_to_transform]\n",
    "document_vectors_bin = [binarize(vec) for vec in document_vectors]\n",
    "\n",
    "# Print the retrieved documents and their count vectors\n",
    "for doc, vec in zip(documents_to_transform, document_vectors):\n",
    "    print(doc)\n",
    "    print(vec.todense())\n",
    "\n",
    "# Calculate and print cosine similarities between document pairs\n",
    "cos_similarities = {\n",
    "    (i + 1, j + 1): cosine_similarity(document_vectors[i], document_vectors[j]).squeeze()\n",
    "    for i in range(len(document_vectors)) for j in range(len(document_vectors))\n",
    "}\n",
    "\n",
    "for key, value in cos_similarities.items():\n",
    "    print(f\"Cosine Similarity using count between {key[0]} and {key[1]}: {value:.6f}\")\n",
    "\n",
    "# Retrieve 3 random sentences, transform them into vectors, binarize the vectors, and calculate cosine similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate a similarity matrix using cosine similarity for all the pairs of term-frequency-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "df = tdm_df.copy()\n",
    "df['cat'] = X['category_name']\n",
    "df.sort_values('cat', inplace=True)\n",
    "\n",
    "# Create index \n",
    "idx = [f\"{i}-{df.loc[i, 'cat']}\" for i in df.index]\n",
    "df = df.drop('cat', axis=1)\n",
    "\n",
    "# Calculate similarity\n",
    "result = pd.DataFrame(\n",
    "    cosine_similarity(df.values), \n",
    "    columns=idx, \n",
    "    index=idx\n",
    ")\n",
    "\n",
    "ta.plot_paginated_heatmap(result)  # Display similarity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate a similarity matrix using cosine similarity for all the pairs of tf-idf-based features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.  Data Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map numerical labels to their corresponding category names\n",
    "category_mapping = dict(X[['category', 'category_name']].drop_duplicates().values)\n",
    "target_names = [category_mapping[label] for label in sorted(category_mapping.keys())]\n",
    "\n",
    "# Train and evaluate on original data\n",
    "X_train, X_test, y_train, y_test = train_test_split(tdm_df, X['category'], test_size=0.3, random_state=10)\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "print(\"Accuracy (Original Data):\", accuracy_score(y_test, y_pred))\n",
    "print(\"Report   (Original Data):\\n\", classification_report(y_test, y_pred, target_names=target_names, digits=4))\n",
    "\n",
    "# Train and evaluate on augmented data\n",
    "X_train, X_test, y_train, y_test = train_test_split(augmented_tdm_df, X['category'], test_size=0.3, random_state=10)\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "print(\"Accuracy (Augmented Data):\", accuracy_score(y_test, y_pred))\n",
    "print(\"Report   (Augmented Data):\\n\", classification_report(y_test, y_pred, target_names=target_names, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
