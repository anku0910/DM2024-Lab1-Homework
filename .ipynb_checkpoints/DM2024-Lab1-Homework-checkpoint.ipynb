{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:龔祺竣\n",
    "\n",
    "Student ID:D1124181010\n",
    "\n",
    "GitHub ID:anku0910"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: do the **take home** exercises in the [DM2024-Lab1-Master](https://github.com/didiersalazar/DM2024-Lab1-Master.git). You may need to copy some cells from the Lab notebook to this notebook. __This part is worth 20% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: follow the same process from the [DM2024-Lab1-Master](https://github.com/didiersalazar/DM2024-Lab1-Master.git) on **the new dataset**. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 30% of your grade.__\n",
    "    - Download the [the new dataset](https://huggingface.co/datasets/Senem/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data). The dataset contains a `sentiment` and `comment` columns, with the sentiment labels being: 'nostalgia' and 'not nostalgia'. Read the specificiations of the dataset for background details. \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 30% of your grade.__\n",
    "    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency). Refer to this Scikit-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n",
    "    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Note that for the TF-IDF features you might need to use other type of NB classifier different than the one in the Master Notebook. Comment on the differences.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n",
    "\n",
    "\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be handled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "5. Fifth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/didiersalazar/DM2024-Lab1-Master/blob/main/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb). Make sure to commit and save your changes to your repository __BEFORE the deadline (October 27th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T23:50:57.474287Z",
     "start_time": "2024-10-11T23:50:57.461278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['He was a singer with a golden voice that I love to hear all the time. I was his great fan at the age of 16years in those days and still now. Although we have many singers now, but, I can vouch for Jim Reeves all the time. You feel relaxed, emotional and loving. Thank you Lord for his life.',\n",
       "       'The mist beautiful voice ever I listened to him when I was a kid and still love his singing Never forgotten The voice of an angel'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "data = pd.read_csv('senem/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data.csv')  # Load\n",
    "categories = data['sentiment'].unique()  # Extract sentiment categories\n",
    "\n",
    "data['sentiment'] = pd.Categorical(data['sentiment'], categories=categories)  # Convert sentiment column to categorical\n",
    "data['sentiment'] = data['sentiment'].cat.codes  # Convert to numerical code\n",
    "\n",
    "X = data['comment'].values \n",
    "y = data['sentiment'].values  \n",
    "\n",
    "sentiment1 = Bunch(data=X, target=y, feature_names=['text'], target_names=categories)  # Create a structured dataset for view\n",
    "sentiment1.data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not nostalgia\n",
      "nostalgia\n",
      "nostalgia\n",
      "not nostalgia\n",
      "nostalgia\n",
      "nostalgia\n",
      "nostalgia\n",
      "nostalgia\n",
      "not nostalgia\n",
      "not nostalgia\n"
     ]
    }
   ],
   "source": [
    "for t in sentiment1.target[:10]:\n",
    "    print(sentiment1.target_names[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiment1.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 1 (Watch Video):**  \n",
    "In this exercise, please print out the *text* data for the first three samples in the dataset. (See the above code for help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was a singer with a golden voice that I love to hear all the time. I was his great fan at the age of 16years in those days and still now. Although we have many singers now, but, I can vouch for Jim Reeves all the time. You feel relaxed, emotional and loving. Thank you Lord for his life.\n",
      "The mist beautiful voice ever I listened to him when I was a kid and still love his singing Never forgotten The voice of an angel\n",
      "I have most of Mr. Reeves songs.  Always love his smooth voice, so comforting and sounds like the people I grew up around.  They were so gentle and sweet, but I had to leave my little town to find a job at age 17.  I graduated early.  I never forgot how great it was to grow up with simple people who had no pretensions -- just loving and gentle, like I think Jesus is.\n"
     ]
    }
   ],
   "source": [
    "# Answer here\n",
    "for text in sentiment1.data[:3]:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Converting Dictionary into Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T23:50:57.599198Z",
     "start_time": "2024-10-11T23:50:57.592204Z"
    }
   },
   "outputs": [],
   "source": [
    "import helpers.data_mining_helpers as dmh\n",
    "\n",
    "X = pd.DataFrame.from_records(dmh.format_rows(sentiment1), columns= ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was a singer with a golden voice that I lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The mist beautiful voice ever I listened to hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have most of Mr. Reeves songs.  Always love ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30 day leave from 1st tour in Viet Nam to conv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>listening to his songs reminds me of my mum wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Every time I heard this song as a child, I use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>My dad loved listening to Jim Reeves, when I w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i HAVE ALSO LISTENED TO Jim Reeves since child...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  He was a singer with a golden voice that I lov...\n",
       "1  The mist beautiful voice ever I listened to hi...\n",
       "2  I have most of Mr. Reeves songs.  Always love ...\n",
       "3  30 day leave from 1st tour in Viet Nam to conv...\n",
       "4  listening to his songs reminds me of my mum wh...\n",
       "5  Every time I heard this song as a child, I use...\n",
       "6  My dad loved listening to Jim Reeves, when I w...\n",
       "7  i HAVE ALSO LISTENED TO Jim Reeves since child..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T23:50:57.646326Z",
     "start_time": "2024-10-11T23:50:57.632317Z"
    }
   },
   "outputs": [],
   "source": [
    "X['category'] = sentiment1.target\n",
    "X['category_name'] = X.category.apply(lambda t: dmh.format_labels(t, sentiment1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T23:50:57.708289Z",
     "start_time": "2024-10-11T23:50:57.681287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was a singer with a golden voice that I lov...</td>\n",
       "      <td>0</td>\n",
       "      <td>not nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The mist beautiful voice ever I listened to hi...</td>\n",
       "      <td>1</td>\n",
       "      <td>nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have most of Mr. Reeves songs.  Always love ...</td>\n",
       "      <td>1</td>\n",
       "      <td>nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30 day leave from 1st tour in Viet Nam to conv...</td>\n",
       "      <td>0</td>\n",
       "      <td>not nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>listening to his songs reminds me of my mum wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Every time I heard this song as a child, I use...</td>\n",
       "      <td>1</td>\n",
       "      <td>nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>My dad loved listening to Jim Reeves, when I w...</td>\n",
       "      <td>1</td>\n",
       "      <td>nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i HAVE ALSO LISTENED TO Jim Reeves since child...</td>\n",
       "      <td>1</td>\n",
       "      <td>nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wherever you  are you always in my heart</td>\n",
       "      <td>0</td>\n",
       "      <td>not nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Elvis will always be number one no one can com...</td>\n",
       "      <td>0</td>\n",
       "      <td>not nostalgia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category  category_name\n",
       "0  He was a singer with a golden voice that I lov...         0  not nostalgia\n",
       "1  The mist beautiful voice ever I listened to hi...         1      nostalgia\n",
       "2  I have most of Mr. Reeves songs.  Always love ...         1      nostalgia\n",
       "3  30 day leave from 1st tour in Viet Nam to conv...         0  not nostalgia\n",
       "4  listening to his songs reminds me of my mum wh...         1      nostalgia\n",
       "5  Every time I heard this song as a child, I use...         1      nostalgia\n",
       "6  My dad loved listening to Jim Reeves, when I w...         1      nostalgia\n",
       "7  i HAVE ALSO LISTENED TO Jim Reeves since child...         1      nostalgia\n",
       "8           Wherever you  are you always in my heart         0  not nostalgia\n",
       "9  Elvis will always be number one no one can com...         0  not nostalgia"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Familiarizing yourself with the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was a singer with a golden voice that I lov...</td>\n",
       "      <td>not nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The mist beautiful voice ever I listened to hi...</td>\n",
       "      <td>nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have most of Mr. Reeves songs.  Always love ...</td>\n",
       "      <td>nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30 day leave from 1st tour in Viet Nam to conv...</td>\n",
       "      <td>not nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>listening to his songs reminds me of my mum wh...</td>\n",
       "      <td>nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Every time I heard this song as a child, I use...</td>\n",
       "      <td>nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>My dad loved listening to Jim Reeves, when I w...</td>\n",
       "      <td>nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i HAVE ALSO LISTENED TO Jim Reeves since child...</td>\n",
       "      <td>nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wherever you  are you always in my heart</td>\n",
       "      <td>not nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Elvis will always be number one no one can com...</td>\n",
       "      <td>not nostalgia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category_name\n",
       "0  He was a singer with a golden voice that I lov...  not nostalgia\n",
       "1  The mist beautiful voice ever I listened to hi...      nostalgia\n",
       "2  I have most of Mr. Reeves songs.  Always love ...      nostalgia\n",
       "3  30 day leave from 1st tour in Viet Nam to conv...  not nostalgia\n",
       "4  listening to his songs reminds me of my mum wh...      nostalgia\n",
       "5  Every time I heard this song as a child, I use...      nostalgia\n",
       "6  My dad loved listening to Jim Reeves, when I w...      nostalgia\n",
       "7  i HAVE ALSO LISTENED TO Jim Reeves since child...      nostalgia\n",
       "8           Wherever you  are you always in my heart  not nostalgia\n",
       "9  Elvis will always be number one no one can com...  not nostalgia"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a simple query\n",
    "X[:10][[\"text\",\"category_name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>He really isn't my cup of tea, but some of his...</td>\n",
       "      <td>0</td>\n",
       "      <td>not nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>real music...how i miss those days...brings ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>i used to cry over this song cause i had a boy...</td>\n",
       "      <td>1</td>\n",
       "      <td>nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>He really has that \"love you like a brother\" l...</td>\n",
       "      <td>0</td>\n",
       "      <td>not nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>That's funny. But Leo really is a nice guy. He...</td>\n",
       "      <td>0</td>\n",
       "      <td>not nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>i don't know!..but the opening of the video,.....</td>\n",
       "      <td>0</td>\n",
       "      <td>not nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>it's sad this is such a beautiful song when yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>not nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>Dear Friend, I think age and time is not that ...</td>\n",
       "      <td>0</td>\n",
       "      <td>not nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>I was born in 1954 and started to be aware of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>nostalgia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>This is the first CD I bought after my marriag...</td>\n",
       "      <td>1</td>\n",
       "      <td>nostalgia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  category  \\\n",
       "1490  He really isn't my cup of tea, but some of his...         0   \n",
       "1491  real music...how i miss those days...brings ba...         1   \n",
       "1492  i used to cry over this song cause i had a boy...         1   \n",
       "1493  He really has that \"love you like a brother\" l...         0   \n",
       "1494  That's funny. But Leo really is a nice guy. He...         0   \n",
       "1495  i don't know!..but the opening of the video,.....         0   \n",
       "1496  it's sad this is such a beautiful song when yo...         0   \n",
       "1497  Dear Friend, I think age and time is not that ...         0   \n",
       "1498  I was born in 1954 and started to be aware of ...         1   \n",
       "1499  This is the first CD I bought after my marriag...         1   \n",
       "\n",
       "      category_name  \n",
       "1490  not nostalgia  \n",
       "1491      nostalgia  \n",
       "1492      nostalgia  \n",
       "1493  not nostalgia  \n",
       "1494  not nostalgia  \n",
       "1495  not nostalgia  \n",
       "1496  not nostalgia  \n",
       "1497  not nostalgia  \n",
       "1498      nostalgia  \n",
       "1499      nostalgia  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     He was a singer with a golden voice that I lov...\n",
       "1     The mist beautiful voice ever I listened to hi...\n",
       "2     I have most of Mr. Reeves songs.  Always love ...\n",
       "3     30 day leave from 1st tour in Viet Nam to conv...\n",
       "4     listening to his songs reminds me of my mum wh...\n",
       "5     Every time I heard this song as a child, I use...\n",
       "6     My dad loved listening to Jim Reeves, when I w...\n",
       "7     i HAVE ALSO LISTENED TO Jim Reeves since child...\n",
       "8              Wherever you  are you always in my heart\n",
       "9     Elvis will always be number one no one can com...\n",
       "10    ill bet if they begin to play this song on the...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using loc (by label)\n",
    "X.loc[:10, 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 2 (take home):** \n",
    "Experiment with other querying techniques using pandas dataframes. Refer to their [documentation](https://pandas.pydata.org/pandas-docs/stable/indexing.html) for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  category  category_name\n",
      "0   He was a singer with a golden voice that I lov...         0  not nostalgia\n",
      "24  I like the part I'm a steam roller baby guaran...         0  not nostalgia\n"
     ]
    }
   ],
   "source": [
    "#Answer here\n",
    "print(X[X['category_name']!='nostalgia'].iloc[::10][0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 3 (Watch Video):**  \n",
    "Try to fetch records belonging to the ```sci.med``` category, and query every 10th record. Only show the first 5 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  category category_name\n",
      "1   The mist beautiful voice ever I listened to hi...         1     nostalgia\n",
      "18  My Grandparents had a pub in the 1970's, this ...         1     nostalgia\n",
      "32  I remember my Mom listening to Jim over and ov...         1     nostalgia\n",
      "50  If I remember correctly, this song came out af...         1     nostalgia\n",
      "71  Gosh does this bring back memories. Nearly eve...         1     nostalgia\n"
     ]
    }
   ],
   "source": [
    "# Answer here\n",
    "print(X[X['category_name']=='nostalgia'].iloc[::10][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Mining using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T23:50:57.817240Z",
     "start_time": "2024-10-11T23:50:57.803247Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The amoung of missing records is:</td>\n",
       "      <td>The amoung of missing records is:</td>\n",
       "      <td>The amoung of missing records is:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text                            category  \\\n",
       "0  The amoung of missing records is:   The amoung of missing records is:    \n",
       "1                                   0                                   0   \n",
       "\n",
       "                        category_name  \n",
       "0  The amoung of missing records is:   \n",
       "1                                   0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 4 (Watch Video):** \n",
    "Let's try something different. Instead of calculating missing values by column let's try to calculate the missing values in every record instead of every column.  \n",
    "$Hint$ : `axis` parameter. Check the documentation for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       (The amoung of missing records is: , 0)\n",
       "1       (The amoung of missing records is: , 0)\n",
       "2       (The amoung of missing records is: , 0)\n",
       "3       (The amoung of missing records is: , 0)\n",
       "4       (The amoung of missing records is: , 0)\n",
       "                         ...                   \n",
       "1495    (The amoung of missing records is: , 0)\n",
       "1496    (The amoung of missing records is: , 0)\n",
       "1497    (The amoung of missing records is: , 0)\n",
       "1498    (The amoung of missing records is: , 0)\n",
       "1499    (The amoung of missing records is: , 0)\n",
       "Length: 1500, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer here\n",
    "X.isnull().apply(lambda X: dmh.check_missing_values(X),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 5 (take home)** \n",
    "$Hint$ :  why `.isnull()` didn't work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Dealing with Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "1495    False\n",
       "1496    False\n",
       "1497    False\n",
       "1498    False\n",
       "1499    False\n",
       "Length: 1500, dtype: bool"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T23:50:57.957159Z",
     "start_time": "2024-10-11T23:50:57.945167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(X.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(X.duplicated('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T23:50:58.128062Z",
     "start_time": "2024-10-11T23:50:58.108073Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T23:50:58.285041Z",
     "start_time": "2024-10-11T23:50:58.267722Z"
    }
   },
   "outputs": [],
   "source": [
    "X.drop_duplicates(keep=\"first\", inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T23:50:58.378978Z",
     "start_time": "2024-10-11T23:50:58.365986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1499"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(X.duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_name\n",
       "nostalgia        750\n",
       "not nostalgia    749\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.category_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_resampled = X.sample(n=450)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to plot the category distribution for both original and resampled data \n",
    "def plot_category_distribution(original_data, resampled_data):\n",
    "    \"\"\" \n",
    "    Function to plot the category distribution for both original and resampled data \n",
    "    \"\"\"\n",
    "    # Convert data to DataFrame\n",
    "    original_df = pd.DataFrame(original_data, columns=[\"category\"])\n",
    "    resampled_df = pd.DataFrame(resampled_data, columns=[\"category\"])\n",
    "\n",
    "    # Calculate value counts for original and resampled data\n",
    "    original_count = original_df['category'].value_counts()\n",
    "    resampled_count = resampled_df['category'].value_counts()\n",
    "\n",
    "    # Define categories and index for plotting\n",
    "    categories = original_count.index.tolist()  # Extract categories from the data\n",
    "    index = np.arange(len(categories))\n",
    "\n",
    "    # Create a bar plot for original and resampled data\n",
    "    plt.bar(index, original_count.values, width=0.2, label='Original', align='center')\n",
    "    plt.bar(index + 0.2, resampled_count.values, width=0.2, label='Resampled', align='center')\n",
    "\n",
    "    # Set x-axis ticks and labels\n",
    "    plt.xticks(index + 0.15, categories)\n",
    "    plt.xlabel('Categories')\n",
    "    plt.title('Category Distribution: Original vs Resampled')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_category_distribution(X, X_resampled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['he',\n",
       "  'was',\n",
       "  'singer',\n",
       "  'with',\n",
       "  'golden',\n",
       "  'voice',\n",
       "  'that',\n",
       "  'love',\n",
       "  'to',\n",
       "  'hear',\n",
       "  'all',\n",
       "  'the',\n",
       "  'time',\n",
       "  'was',\n",
       "  'his',\n",
       "  'great',\n",
       "  'fan',\n",
       "  'at',\n",
       "  'the',\n",
       "  'age',\n",
       "  'of',\n",
       "  '16years',\n",
       "  'in',\n",
       "  'those',\n",
       "  'days',\n",
       "  'and',\n",
       "  'still',\n",
       "  'now',\n",
       "  'although',\n",
       "  'we',\n",
       "  'have',\n",
       "  'many',\n",
       "  'singers',\n",
       "  'now',\n",
       "  'but',\n",
       "  'can',\n",
       "  'vouch',\n",
       "  'for',\n",
       "  'jim',\n",
       "  'reeves',\n",
       "  'all',\n",
       "  'the',\n",
       "  'time',\n",
       "  'you',\n",
       "  'feel',\n",
       "  'relaxed',\n",
       "  'emotional',\n",
       "  'and',\n",
       "  'loving',\n",
       "  'thank',\n",
       "  'you',\n",
       "  'lord',\n",
       "  'for',\n",
       "  'his',\n",
       "  'life']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "ngram_range=(1,1) \n",
    "X['unigrams'] = X['text'].apply(CountVectorizer(ngram_range=ngram_range).build_analyzer()) \n",
    "# Use built-in tokenizer \n",
    "list(X[0:1]['unigrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_name</th>\n",
       "      <th>unigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was a singer with a golden voice that I lov...</td>\n",
       "      <td>0</td>\n",
       "      <td>not nostalgia</td>\n",
       "      <td>[he, was, singer, with, golden, voice, that, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The mist beautiful voice ever I listened to hi...</td>\n",
       "      <td>1</td>\n",
       "      <td>nostalgia</td>\n",
       "      <td>[the, mist, beautiful, voice, ever, listened, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have most of Mr. Reeves songs.  Always love ...</td>\n",
       "      <td>1</td>\n",
       "      <td>nostalgia</td>\n",
       "      <td>[have, most, of, mr, reeves, songs, always, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30 day leave from 1st tour in Viet Nam to conv...</td>\n",
       "      <td>0</td>\n",
       "      <td>not nostalgia</td>\n",
       "      <td>[30, day, leave, from, 1st, tour, in, viet, na...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category  category_name  \\\n",
       "0  He was a singer with a golden voice that I lov...         0  not nostalgia   \n",
       "1  The mist beautiful voice ever I listened to hi...         1      nostalgia   \n",
       "2  I have most of Mr. Reeves songs.  Always love ...         1      nostalgia   \n",
       "3  30 day leave from 1st tour in Viet Nam to conv...         0  not nostalgia   \n",
       "\n",
       "                                            unigrams  \n",
       "0  [he, was, singer, with, golden, voice, that, l...  \n",
       "1  [the, mist, beautiful, voice, ever, listened, ...  \n",
       "2  [have, most, of, mr, reeves, songs, always, lo...  \n",
       "3  [30, day, leave, from, 1st, tour, in, viet, na...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Feature subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plot_x = [\"term_\" + str(i) for i in vectorizer.get_feature_names_out()[0:20]]\n",
    "plot_y = [\"doc_\" + str(i) for i in list(X.index)[0:20]]\n",
    "plot_z = tdm[0:20, 0:20].toarray()  # Select first 20 documents and first 20 terms\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "tdm_df = pd.DataFrame(plot_z, columns=plot_x, index=plot_y)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(tdm_df, cmap='RdPu', vmin=0, vmax=tdm_df.max().max(), annot=False, cbar_kws={'label': 'Term Frequency'})\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Atrribute Transformation / Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers.text_analysis as ta\n",
    "\n",
    "# In the helpers.text_analysis package, I designed a function plot_term_frequencies\n",
    "# to draw a term frequencies with pagination, given a term-document matrix\n",
    "ta.plot_term_frequencies(tdm_df, start_range=0, end_range=100, logScale=False, ascending=None)\n",
    "ta.plot_term_frequencies(tdm_df, start_range=0, end_range=100, logScale=True, ascending=None)\n",
    "ta.plot_term_frequencies(tdm_df, start_range=0, end_range=100, logScale=False, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding frequent patterns for each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting texts for each category to a term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Create separate DataFrames for each category\n",
    "category_dfs = {}  # Dictionary to store DataFrames for each category\n",
    "for category in categories:\n",
    "    # Filter the original DataFrame by category\n",
    "    category_dfs[category] = X[X['category_name'] == category].copy()\n",
    "\n",
    "# Create term-document frequency DataFrames for each category\n",
    "term_document_dfs = {}  # Dictionary to store term-document DataFrames for each category\n",
    "for category in categories:\n",
    "    term_document_dfs[category] = dmh.create_term_document_df(category_dfs[category]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "category_number = 0\n",
    "category_name = categories[category_number]\n",
    "category_word_counts = term_document_dfs[category_name].sum(axis=0).to_numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(category_word_counts, bins=700, color='blue', edgecolor='black')\n",
    "plt.title(f'Term Frequency Distribution for Category {category_name}')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Number of Terms')\n",
    "plt.xlim(1, 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering the bottom 1% and top 5% words from the term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filtering function to each category texts\n",
    "filt_term_document_dfs = {}\n",
    "stop_words_dict = {} # collect those removed words as stop_words for CountVectorizer/TfidfVectorizer if necessary\n",
    "for category in categories:\n",
    "    # print(f'\\nFor category {category} we filter the following words:')\n",
    "    filt_term_document_dfs[category], stop_words_dict[category] = dmh.filter_top_bottom_words_by_sum(term_document_dfs[category], verbose=False)\n",
    "    \n",
    "# Combine all removed words lists into one, which is a list of stop words for the subsequent process (if necessary)\n",
    "stop_words_list = [item for sublist in stop_words_dict.values() for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category_name in categories:  #You can change it from 0 to 1\n",
    "    print(f\"Plot of Term Frequencies for Category {category_name}\")\n",
    "    ta.plot_term_frequencies(filt_term_document_dfs[category_name], start_range=0, end_range=100, logScale=False, ascending=False)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 16 (take home):** \n",
    "Review the words that were filtered in each category and comment about the differences and similarities that you can see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Step 1: Load data from files\n",
    "def load_patterns(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        patterns = file.read().splitlines()\n",
    "    return [pattern.split(\":\")[0].strip() for pattern in patterns]\n",
    "\n",
    "# Load patterns from both files\n",
    "nostalgia_patterns = load_patterns(\"freq_patterns_nostalgia_minSup9.txt\")\n",
    "not_nostalgia_patterns = load_patterns(\"freq_patterns_not_nostalgia_minSup9.txt\")\n",
    "\n",
    "# Step 2: Identify common and unique words\n",
    "common_words = set(nostalgia_patterns) & set(not_nostalgia_patterns)\n",
    "unique_nostalgia = set(nostalgia_patterns) - common_words\n",
    "unique_not_nostalgia = set(not_nostalgia_patterns) - common_words\n",
    "\n",
    "# Step 3: Frequency analysis for emotional, temporal, and descriptive words\n",
    "emotional_words = [\"cry\", \"tears\", \"memory\", \"gone\", \"feel\", \"happy\", \"wonderful\", \"sad\", \"amazing\"]\n",
    "temporal_words = [\"youth\", \"yesterday\", \"growing\", \"1963\", \"today\", \"new\", \"yesterday\", \"forever\", \"before\"]\n",
    "descriptive_adjectives = [\"classic\", \"fantastic\", \"sweet\", \"special\", \"brilliant\", \"lovely\"]\n",
    "\n",
    "# Count occurrences in both sets\n",
    "def word_category_analysis(patterns, words):\n",
    "    counter = Counter(patterns)\n",
    "    return {word: counter[word] for word in words if word in counter}\n",
    "\n",
    "nostalgia_emotions = word_category_analysis(nostalgia_patterns, emotional_words)\n",
    "not_nostalgia_emotions = word_category_analysis(not_nostalgia_patterns, emotional_words)\n",
    "\n",
    "nostalgia_temporal = word_category_analysis(nostalgia_patterns, temporal_words)\n",
    "not_nostalgia_temporal = word_category_analysis(not_nostalgia_patterns, temporal_words)\n",
    "\n",
    "nostalgia_descriptions = word_category_analysis(nostalgia_patterns, descriptive_adjectives)\n",
    "not_nostalgia_descriptions = word_category_analysis(not_nostalgia_patterns, descriptive_adjectives)\n",
    "\n",
    "# Step 4: Visualizations\n",
    "def plot_wordcloud(word_dict, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_dict)\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Generate word clouds for analysis\n",
    "plot_wordcloud(nostalgia_emotions, \"Nostalgia - Emotional Words\")\n",
    "plot_wordcloud(not_nostalgia_emotions, \"Not Nostalgia - Emotional Words\")\n",
    "plot_wordcloud(nostalgia_temporal, \"Nostalgia - Temporal References\")\n",
    "plot_wordcloud(not_nostalgia_temporal, \"Not Nostalgia - Temporal References\")\n",
    "plot_wordcloud(nostalgia_descriptions, \"Nostalgia - Descriptive Adjectives\")\n",
    "plot_wordcloud(not_nostalgia_descriptions, \"Not Nostalgia - Descriptive Adjectives\")\n",
    "\n",
    "# Step 5: Display Findings\n",
    "print(\"Common Words:\", len(common_words))\n",
    "print(\"Unique Nostalgia Words:\", len(unique_nostalgia))\n",
    "print(\"Unique Not Nostalgia Words:\", len(unique_not_nostalgia))\n",
    "\n",
    "print(\"\\nNostalgia - Emotional Words:\", nostalgia_emotions)\n",
    "print(\"Not Nostalgia - Emotional Words:\", not_nostalgia_emotions)\n",
    "\n",
    "print(\"\\nNostalgia - Temporal References:\", nostalgia_temporal)\n",
    "print(\"Not Nostalgia - Temporal References:\", not_nostalgia_temporal)\n",
    "\n",
    "print(\"\\nNostalgia - Descriptive Adjectives:\", nostalgia_descriptions)\n",
    "print(\"Not Nostalgia - Descriptive Adjectives:\", not_nostalgia_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import wordnet\n",
    "from nrclex import NRCLex\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# Make sure to download the necessary NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "# nltk.download('all') # totally around 3.4 GB\n",
    "\n",
    "# Step 1: Load data from files (same as previous code)\n",
    "def load_patterns(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        patterns = file.read().splitlines()\n",
    "    return [pattern.split(\":\")[0].strip() for pattern in patterns]\n",
    "\n",
    "# Load patterns from both files\n",
    "nostalgia_patterns = load_patterns(\"freq_patterns_nostalgia_minSup9.txt\")\n",
    "not_nostalgia_patterns = load_patterns(\"freq_patterns_not_nostalgia_minSup9.txt\")\n",
    "\n",
    "# Step 2: Enhanced word category analysis using NRCLex for emotional words\n",
    "def categorize_with_nrclex(word_list):\n",
    "    emotions_dict = {'positive': [], 'negative': [], 'joy': [], 'sadness': [], 'anger': [], 'anticipation': [], 'trust': [], 'fear': [], 'surprise': [], 'disgust': []}\n",
    "    for word in word_list:\n",
    "        analysis = NRCLex(word)\n",
    "        for emotion in emotions_dict.keys():\n",
    "            if emotion in analysis.raw_emotion_scores:\n",
    "                emotions_dict[emotion].append(word)\n",
    "    return {k: len(v) for k, v in emotions_dict.items()}\n",
    "\n",
    "# Step 3: Temporal Tagging - Manually defined list\n",
    "temporal_words = [\n",
    "    \"yesterday\", \"today\", \"tomorrow\", \"morning\", \"evening\", \"night\", \"week\", \"month\", \"year\", \"decade\", \n",
    "    \"century\", \"past\", \"present\", \"future\", \"now\", \"then\", \"before\", \"after\", \"early\", \"late\", \"1963\", \n",
    "    \"youth\", \"forever\", \"years\", \"months\", \"days\", \"once\", \"ago\"\n",
    "]\n",
    "\n",
    "def count_temporal_words(patterns, temporal_list):\n",
    "    return {word: patterns.count(word) for word in temporal_list if word in patterns}\n",
    "\n",
    "nostalgia_temporal = count_temporal_words(nostalgia_patterns, temporal_words)\n",
    "not_nostalgia_temporal = count_temporal_words(not_nostalgia_patterns, temporal_words)\n",
    "\n",
    "# Step 4: Extract Descriptive Adjectives Using POS Tagging\n",
    "def extract_adjectives(word_list):\n",
    "    adjectives = []\n",
    "    for pattern in word_list:\n",
    "        tokens = word_tokenize(pattern)\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        adjectives.extend([word for word, tag in pos_tags if tag == 'JJ'])  # 'JJ' is the POS tag for adjectives\n",
    "    return Counter(adjectives)\n",
    "\n",
    "nostalgia_adjectives = extract_adjectives(nostalgia_patterns)\n",
    "not_nostalgia_adjectives = extract_adjectives(not_nostalgia_patterns)\n",
    "\n",
    "# Step 5: Visualizations and Findings\n",
    "def plot_wordcloud(word_dict, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_dict)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Generate word clouds for analysis\n",
    "plot_wordcloud(nostalgia_temporal, \"Nostalgia - Temporal References\")\n",
    "plot_wordcloud(not_nostalgia_temporal, \"Not Nostalgia - Temporal References\")\n",
    "\n",
    "plot_wordcloud(nostalgia_adjectives, \"Nostalgia - Descriptive Adjectives\")\n",
    "plot_wordcloud(not_nostalgia_adjectives, \"Not Nostalgia - Descriptive Adjectives\")\n",
    "\n",
    "# Display Findings\n",
    "print(\"Nostalgia Emotional Words:\", categorize_with_nrclex(nostalgia_patterns))\n",
    "print(\"Not Nostalgia Emotional Words:\", categorize_with_nrclex(not_nostalgia_patterns))\n",
    "\n",
    "print(\"\\nNostalgia - Temporal References:\", nostalgia_temporal)\n",
    "print(\"Not Nostalgia - Temporal References:\", not_nostalgia_temporal)\n",
    "\n",
    "print(\"\\nNostalgia - Descriptive Adjectives:\", nostalgia_adjectives.most_common(10))\n",
    "print(\"Not Nostalgia - Descriptive Adjectives:\", not_nostalgia_adjectives.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the filtered term-document matrix to transactional database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PAMI.extras.DF2DB import DenseFormatDF as db\n",
    "\n",
    "# Loop through the dictionary of term-document DataFrames\n",
    "for category in filt_term_document_dfs:\n",
    "    # Replace dots with underscores in the category name to avoid errors in the file creation\n",
    "    category_safe = category.replace(' ', '_')\n",
    "    \n",
    "    # Create the DenseFormatDF object and convert to a transactional database\n",
    "    obj = db.DenseFormatDF(filt_term_document_dfs[category])\n",
    "    obj.convert2TransactionalDatabase(f'td_freq_db_{category_safe}.csv', '>=', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing and visualizing stats for transactional database (for determining support threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PAMI.extras.dbStats import TransactionalDatabase as tds\n",
    "obj = tds.TransactionalDatabase('td_freq_db_not_nostalgia.csv')\n",
    "obj.run()\n",
    "obj.printStats()\n",
    "obj.plotGraphs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PAMI.extras.dbStats import TransactionalDatabase as tds\n",
    "obj = tds.TransactionalDatabase('td_freq_db_nostalgia.csv')\n",
    "obj.run()\n",
    "obj.printStats()\n",
    "obj.plotGraphs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply FPGrowth algorithms to finding frequent patterns for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PAMI.frequentPattern.basic import FPGrowth as alg\n",
    "minSup=9\n",
    "obj1 = alg.FPGrowth(iFile='td_freq_db_not_nostalgia.csv', minSup=minSup)\n",
    "obj1.mine()\n",
    "frequentPatternsDF_not_nostalgia = obj1.getPatternsAsDataFrame()\n",
    "print('Total No of patterns: ' + str(len(frequentPatternsDF_not_nostalgia))) #print the total number of patterns\n",
    "print('Runtime: ' + str(obj1.getRuntime())) #measure the runtime\n",
    "obj1.save('freq_patterns_not_nostalgia_minSup9.txt') #save the patterns\n",
    "\n",
    "print('Plot of Frequent Patterns for Category not_nostalgia')\n",
    "ta.plot_frequent_patterns(frequentPatternsDF_not_nostalgia, start_range=0, end_range=50, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PAMI.frequentPattern.basic import FPGrowth as alg\n",
    "minSup=9\n",
    "obj1 = alg.FPGrowth(iFile='td_freq_db_nostalgia.csv', minSup=minSup)\n",
    "obj1.mine()\n",
    "frequentPatternsDF_nostalgia = obj1.getPatternsAsDataFrame()\n",
    "print('Total No of patterns: ' + str(len(frequentPatternsDF_nostalgia))) #print the total number of patterns\n",
    "print('Runtime: ' + str(obj1.getRuntime())) #measure the runtime\n",
    "obj1.save('freq_patterns_nostalgia_minSup9.txt') #save the patterns\n",
    "\n",
    "print('Plot of Frequent Patterns for Category nostalgia')\n",
    "ta.plot_frequent_patterns(frequentPatternsDF_nostalgia, start_range=0, end_range=50, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each category, filter the patterns to keep only the unique ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#We group together all of the dataframes related to our found patterns\n",
    "dfs = [frequentPatternsDF_not_nostalgia, frequentPatternsDF_nostalgia]\n",
    "\n",
    "# Identify patterns that appear in more than one category\n",
    "# Count how many times each pattern appears across all dataframes\n",
    "pattern_counts = {}\n",
    "for df in dfs:\n",
    "    for pattern in df['Patterns']:\n",
    "        if pattern not in pattern_counts:\n",
    "            pattern_counts[pattern] = 1\n",
    "        else:\n",
    "            pattern_counts[pattern] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out patterns that appear in more than one dataframe\n",
    "unique_patterns = {pattern for pattern, count in pattern_counts.items() if count == 1}\n",
    "len(unique_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of patterns across all categories\n",
    "total_patterns_count = sum(len(df) for df in dfs)\n",
    "# Calculate how many patterns were discarded\n",
    "discarded_patterns_count = total_patterns_count - len(unique_patterns)\n",
    "\n",
    "# For each category, filter the patterns to keep only the unique ones\n",
    "filtered_dfs = []\n",
    "for df in dfs:\n",
    "    filtered_df = df[df['Patterns'].isin(unique_patterns)]\n",
    "    filtered_dfs.append(filtered_df)\n",
    "\n",
    "# Merge the filtered dataframes into a final dataframe\n",
    "final_pattern_df = pd.concat(filtered_dfs, ignore_index=True)\n",
    "\n",
    "# Print the number of discarded patterns\n",
    "print(f\"Number of patterns discarded: {discarded_patterns_count}\")\n",
    "\n",
    "# Display the final pattern result\n",
    "print('Plot of Unique Frequent Patterns')\n",
    "ta.plot_frequent_patterns(final_pattern_df, start_range=0, end_range=50, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the texts into the frequency-based term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# By adding stop_words into CountVectorizer, in this dataset, the dimension can be reduced from 3730 to 3520.\n",
    "# After experiments, this is not helpful for improving the classification performance. As a result, I commented the following line.\n",
    "# vectorizer = CountVectorizer(stop_words=stop_words_list)\n",
    "\n",
    "# Convert 'text' column into term-document matrix using CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "tokenizer = count_vectorizer.build_analyzer()\n",
    "\n",
    "# don't forget the index parameter for consistent indexing, especially for concating two dataframes\n",
    "tdm_df = dmh.create_term_document_df(X['text'], vectorizer=count_vectorizer, index=X.index)\n",
    "tdm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the texts into the TF-IDF-based term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert 'text' column into tf-idf-based term-document matrix using TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# don't forget the index parameter for consistent indexing, especially for concating two dataframes\n",
    "tfidf_tdm_df = dmh.create_term_document_df(X['text'], vectorizer=tfidf_vectorizer, index=X.index)\n",
    "tfidf_tdm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the unique patterns into the 0/1 patterm-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text into words and then convert to a set representation\n",
    "X['tokenized_text'] = X['text'].apply(tokenizer).apply(set)\n",
    "\n",
    "# Initialize the pattern matrix\n",
    "pattern_matrix = pd.DataFrame(0, index=X.index, columns=final_pattern_df['Patterns'])\n",
    "\n",
    "# Iterate over each pattern and check if all words in the pattern are present in the tokenized sentence\n",
    "for pattern in final_pattern_df['Patterns']:\n",
    "    pattern_words = set(pattern.split())  # Tokenize pattern into words and then convert to a set representation\n",
    "    pattern_matrix[pattern] = X['tokenized_text'].apply(lambda x: 1 if pattern_words.issubset(x) else 0)\n",
    "    \n",
    "pattern_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate the frequence-based term-document matrix and the pattern matrix to augment the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the original TDM and the pattern matrix to augment the features\n",
    "augmented_tdm_df = pd.concat([tdm_df, pattern_matrix], axis=1)\n",
    "augmented_tdm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate the tf-idf-based term-document matrix and the pattern matrix to augment the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the original TDM and the pattern matrix to augment the features\n",
    "augmented_tfidf_tdm_df = pd.concat([tfidf_tdm_df, pattern_matrix], axis=1)\n",
    "augmented_tfidf_tdm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 5.5 Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying dimensionality reduction with only the document-term frequency data\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to create a scatter plot for each method\n",
    "def plot_scatter_2d(ax, X_reduced, title, colors, categories):\n",
    "    for c, category in zip(colors, categories):\n",
    "        xs = X_reduced[X['category_name'] == category].T[0]\n",
    "        ys = X_reduced[X['category_name'] == category].T[1]\n",
    "        ax.scatter(xs, ys, c=c, marker='o', label=category)\n",
    "    \n",
    "    ax.grid(color='gray', linestyle=':', linewidth=2, alpha=0.2)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "# Define a function to create a 3D scatter plot for each method\n",
    "def plot_scatter_3d(ax, X_reduced, title, colors, categories):\n",
    "    for c, category in zip(colors, categories):\n",
    "        xs = X_reduced[X['category_name'] == category].T[0]\n",
    "        ys = X_reduced[X['category_name'] == category].T[1]\n",
    "        zs = X_reduced[X['category_name'] == category].T[2]\n",
    "        ax.scatter(xs, ys, zs, c=c, marker='o', label=category)\n",
    "    \n",
    "    ax.grid(color='gray', linestyle=':', linewidth=2, alpha=0.2)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply dimensionality reduction to the frequency-based term-document matrix and keep 2 components only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This might take a couple of minutes to execute\n",
    "# Apply PCA, t-SNE, and UMAP to the data\n",
    "X_pca_tdm = PCA(n_components=2).fit_transform(tdm_df.values)\n",
    "X_tsne_tdm = TSNE(n_components=2).fit_transform(tdm_df.values)\n",
    "X_umap_tdm = umap.UMAP(n_components=2).fit_transform(tdm_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results in subplots\n",
    "colors = ['coral', 'blue', 'black', 'orange']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))  # Create 3 subplots for PCA, t-SNE, and UMAP\n",
    "fig.suptitle('PCA, t-SNE, and UMAP Comparison')\n",
    "\n",
    "plot_scatter_2d(axes[0], X_pca_tdm, 'PCA', colors=colors, categories=categories)\n",
    "plot_scatter_2d(axes[1], X_tsne_tdm, 't-SNE', colors=colors, categories=categories)\n",
    "plot_scatter_2d(axes[2], X_umap_tdm, 'UMAP', colors=colors, categories=categories)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply dimensionality reduction to the augmented frequency-based term-document matrix and keep 2 components only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This might take a couple of minutes to execute\n",
    "# Apply PCA, t-SNE, and UMAP to the data\n",
    "X_pca_tdm_aug = PCA(n_components=2).fit_transform(augmented_tdm_df.values)\n",
    "X_tsne_tdm_aug = TSNE(n_components=2).fit_transform(augmented_tdm_df.values)\n",
    "X_umap_tdm_aug = umap.UMAP(n_components=2).fit_transform(augmented_tdm_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results in subplots\n",
    "colors = ['coral', 'blue', 'black', 'orange']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))  # Create 3 subplots for PCA, t-SNE, and UMAP\n",
    "fig.suptitle('PCA, t-SNE, and UMAP Comparison')\n",
    "\n",
    "plot_scatter_2d(axes[0], X_pca_tdm_aug, 'PCA', colors=colors, categories=categories)\n",
    "plot_scatter_2d(axes[1], X_tsne_tdm_aug, 't-SNE', colors=colors, categories=categories)\n",
    "plot_scatter_2d(axes[2], X_umap_tdm_aug, 'UMAP', colors=colors, categories=categories)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply dimensionality reduction to the frequency-based term-document matrix and keep 3 components only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = PCA(n_components=3).fit_transform(tdm_df.values)\n",
    "X_tsne = TSNE(n_components=3).fit_transform(tdm_df.values)\n",
    "X_umap = umap.UMAP(n_components=3).fit_transform(tdm_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results in subplots\n",
    "fig = plt.figure(figsize=(30, 10))  # Create 3D subplots for PCA, t-SNE, and UMAP\n",
    "fig.suptitle('PCA, t-SNE, and UMAP Comparison')\n",
    "\n",
    "# Create 3D subplots\n",
    "ax_pca = fig.add_subplot(131, projection='3d')\n",
    "ax_tsne = fig.add_subplot(132, projection='3d')\n",
    "ax_umap = fig.add_subplot(133, projection='3d')\n",
    "\n",
    "# Create 3D scatter plots for PCA, t-SNE, and UMAP\n",
    "plot_scatter_3d(ax_pca, X_pca, 'PCA', colors=colors, categories=categories)\n",
    "plot_scatter_3d(ax_tsne, X_tsne, 't-SNE', colors=colors, categories=categories)\n",
    "plot_scatter_3d(ax_umap, X_umap, 'UMAP', colors=colors, categories=categories)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply dimensionality reduction to the augemented term-document matrix and keep 3 components only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_aug = PCA(n_components=3).fit_transform(augmented_tdm_df.values)\n",
    "X_tsne_aug = TSNE(n_components=3).fit_transform(augmented_tdm_df.values)\n",
    "X_umap_aug = umap.UMAP(n_components=3).fit_transform(augmented_tdm_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results in subplots\n",
    "fig = plt.figure(figsize=(30, 10))  # Create 3D subplots for PCA, t-SNE, and UMAP\n",
    "fig.suptitle('PCA, t-SNE, and UMAP Comparison')\n",
    "\n",
    "# Create 3D subplots\n",
    "ax_pca = fig.add_subplot(131, projection='3d')\n",
    "ax_tsne = fig.add_subplot(132, projection='3d')\n",
    "ax_umap = fig.add_subplot(133, projection='3d')\n",
    "\n",
    "# Create 3D scatter plots for PCA, t-SNE, and UMAP\n",
    "plot_scatter_3d(ax_pca, X_pca_aug, 'PCA', colors=colors, categories=categories)\n",
    "plot_scatter_3d(ax_tsne, X_tsne_aug, 't-SNE', colors=colors, categories=categories)\n",
    "plot_scatter_3d(ax_umap, X_umap_aug, 'UMAP', colors=colors, categories=categories)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Discretization and Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, metrics, decomposition, pipeline, dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = preprocessing.LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.fit(X.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['bin_category'] = mlb.transform(X['category']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6.  Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We retrieve 3 sentences for a random record\n",
    "document_to_transform_1 = []\n",
    "random_record_1 = X.iloc[50]\n",
    "random_record_1 = random_record_1['text']\n",
    "document_to_transform_1.append(random_record_1)\n",
    "\n",
    "document_to_transform_2 = []\n",
    "random_record_2 = X.iloc[100]\n",
    "random_record_2 = random_record_2['text']\n",
    "document_to_transform_2.append(random_record_2)\n",
    "\n",
    "document_to_transform_3 = []\n",
    "random_record_3 = X.iloc[150]\n",
    "random_record_3 = random_record_3['text']\n",
    "document_to_transform_3.append(random_record_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document_to_transform_1)\n",
    "print(document_to_transform_2)\n",
    "print(document_to_transform_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "\n",
    "# Transform sentence with Vectorizers\n",
    "document_vector_count_1 = count_vectorizer.transform(document_to_transform_1)\n",
    "document_vector_count_2 = count_vectorizer.transform(document_to_transform_2)\n",
    "document_vector_count_3 = count_vectorizer.transform(document_to_transform_3)\n",
    "\n",
    "# Binarize vectors to simplify: 0 for abscence, 1 for prescence\n",
    "document_vector_count_1_bin = binarize(document_vector_count_1)\n",
    "document_vector_count_2_bin = binarize(document_vector_count_2)\n",
    "document_vector_count_3_bin = binarize(document_vector_count_3)\n",
    "\n",
    "# print vectors\n",
    "print(\"Let's take a look at the count vectors:\")\n",
    "print(document_vector_count_1.todense())\n",
    "print(document_vector_count_2.todense())\n",
    "print(document_vector_count_3.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate Cosine Similarity\n",
    "cos_sim_count_1_2 = cosine_similarity(document_vector_count_1, document_vector_count_2, dense_output=True).squeeze()\n",
    "cos_sim_count_1_3 = cosine_similarity(document_vector_count_1, document_vector_count_3, dense_output=True).squeeze()\n",
    "cos_sim_count_2_3 = cosine_similarity(document_vector_count_2, document_vector_count_3, dense_output=True).squeeze()\n",
    "\n",
    "cos_sim_count_1_1 = cosine_similarity(document_vector_count_1, document_vector_count_1, dense_output=True).squeeze()\n",
    "cos_sim_count_2_2 = cosine_similarity(document_vector_count_2, document_vector_count_2, dense_output=True).squeeze()\n",
    "cos_sim_count_3_3 = cosine_similarity(document_vector_count_3, document_vector_count_3, dense_output=True).squeeze()\n",
    "\n",
    "# Print \n",
    "print(\"Cosine Similarity using count bw 1 and 2: %(x)f\" %{\"x\":cos_sim_count_1_2})\n",
    "print(\"Cosine Similarity using count bw 1 and 3: %(x)f\" %{\"x\":cos_sim_count_1_3})\n",
    "print(\"Cosine Similarity using count bw 2 and 3: %(x)f\" %{\"x\":cos_sim_count_2_3})\n",
    "\n",
    "print(\"Cosine Similarity using count bw 1 and 1: %(x)f\" %{\"x\":cos_sim_count_1_1})\n",
    "print(\"Cosine Similarity using count bw 2 and 2: %(x)f\" %{\"x\":cos_sim_count_2_2})\n",
    "print(\"Cosine Similarity using count bw 3 and 3: %(x)f\" %{\"x\":cos_sim_count_3_3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate a similarity matrix using cosine similarity for all the pairs of term-frequency-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "term_document_matrix = tdm_df.copy() # Replace tdm_df with augmented_tdm_df, tfidf_tdm_df, augmented_tfidf_tdm_df, \n",
    "\n",
    "term_document_matrix['category_name'] = X['category_name']\n",
    "term_document_matrix.sort_values(by='category_name', inplace=True)\n",
    "\n",
    "index_names = [str(doc_id)+'-'+term_document_matrix.loc[doc_id, 'category_name'] \n",
    "               for doc_id in term_document_matrix.index.to_list()]\n",
    "term_document_matrix.drop('category_name', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Step 1: Calculate the Cosine Similarity Matrix\n",
    "similarity_matrix = cosine_similarity(term_document_matrix.values)\n",
    "\n",
    "# Step 2: Convert the similarity matrix of numpy array into a dataframe\n",
    "similarity_df = pd.DataFrame(similarity_matrix, columns=index_names, index=index_names)\n",
    "similarity_df\n",
    "\n",
    "ta.plot_paginated_heatmap(similarity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate a similarity matrix using cosine similarity for all the pairs of tf-idf-based features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.  Data Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with only the frequency-based document-term frequency data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Create a mapping from numerical labels to category names\n",
    "category_mapping = dict(X[['category', 'category_name']].drop_duplicates().values)\n",
    "\n",
    "# Convert the numerical category labels to text labels\n",
    "target_names = [category_mapping[label] for label in sorted(category_mapping.keys())]\n",
    "\n",
    "# Split the data into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(tdm_df, X['category'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with the augmented data where frequency-based tdm and unique-pattern matrix are concated\n",
    "\n",
    "# Create a mapping from numerical labels to category names\n",
    "category_mapping = dict(X[['category', 'category_name']].drop_duplicates().values)\n",
    "\n",
    "# Convert the numerical category labels to text labels\n",
    "target_names = [category_mapping[label] for label in sorted(category_mapping.keys())]\n",
    "\n",
    "# Split the data into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(augmented_tdm_df, X['category'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with the augmented data where tf-idf-based tdm and unique-pattern matrix are concated\n",
    "\n",
    "# Create a mapping from numerical labels to category names\n",
    "category_mapping = dict(X[['category', 'category_name']].drop_duplicates().values)\n",
    "\n",
    "# Convert the numerical category labels to text labels\n",
    "target_names = [category_mapping[label] for label in sorted(category_mapping.keys())]\n",
    "\n",
    "# Split the data into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(augmented_tfidf_tdm_df, X['category'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "nb_classifier = BernoulliNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "tdm_df = pd.DataFrame(X.toarray(), columns=terms)\n",
    "\n",
    "tdm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "tdm_df = pd.DataFrame(X.toarray(), columns=terms)\n",
    "\n",
    "tdm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Note\n",
    "1. Thresholds used to filter out top and bottom occurence words\n",
    "2. Algorithms used to retrieve frequent patterns\n",
    "3. Data leakage problem when applying feature subset selection in the Master program\n",
    "4. Visualizing similarity matrix for the two-class feature sets\n",
    "5. Re-organize the code\n",
    "6. Analyze the frequent patterns\n",
    "7. Design a good classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
